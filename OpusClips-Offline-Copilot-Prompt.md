# OpusClips Offline - VollstÃ¤ndiger GitHub Copilot Prompt

## ğŸ¯ ProjektÃ¼bersicht

Du wirst eine **vollstÃ¤ndig offline funktionierende Desktop-Anwendung** entwickeln, die wie OpusClips funktioniert. Die Anwendung teilt lange Videos automatisch in mehrere kÃ¼rzere, Social-Media-optimierte Clips auf - **ohne Internet und ohne Cloud-Services**.

---

## ğŸ“‹ Kernfeatures

### 1. **Video-Upload & Verwaltung**
- Lokale Video-Datei-Upload (MP4, MKV, WebM)
- Zeige Video-Informationen: Dauer, AuflÃ¶sung, Bitrate, Framerate
- Video-Vorschau in der BenutzeroberflÃ¤che
- Speichere Video-Metadaten lokal

### 2. **Automatische Szenerkennung (Scene Detection)**
- Verwende **PySceneDetect** mit `AdaptiveDetector()`
- Erkenne automatisch Schnitte, ÃœbergÃ¤nge und Szenenwechsel
- **Konfigurierbare Erkennungs-Schwellenwerte:**
  - Scene Sensitivity: 0.1 - 0.9 (Standard: 0.3)
  - Content-basierte Erkennung
  - Adaptive Frameanalyse
- Zeige erkannte Szenen als Timeline mit Thumbnails
- Export erkannter Szenen als Liste

### 3. **Intelligente Clip-Generierung**
- **Automatische Clip-Erstellung:** 
  - Zielclip-LÃ¤nge: 15s, 30s, 60s (konfigurierbar)
  - Mindest-/Maximal-LÃ¤ngen
  - Intelligentes Zusammenfassen von sehr kurzen Szenen
  - Ãœberlappung vermeiden oder hinzufÃ¼gen (konfigurierbar)

- **Intelligente Moment-Erkennung mit Ollama:**
  - Verwende lokales LLM (Ollama) zur Videoanalyse
  - Nutze Whisper (lokal) fÃ¼r Sprach-Transkription
  - Erkenne "Highlight-Momente" basierend auf:
    - SprachintensitÃ¤t (laute, energische Abschnitte)
    - Schnelle Szenenwechsel
    - Musik/Sound-Spitzen
    - Benutzermarker (manuell gesetzt)

### 4. **Clip-Bewertung & Ranking**
- **Automatische Bewertung (1-10 Punkte):**
  - Geschwindigkeit der visuellen Ã„nderungen (Schnitte)
  - Audio-Dynamik (LautstÃ¤rke-Spitzen)
  - SzenenkomplexitÃ¤t
  - Manuell anpassbar durch Slider
- **Filtern nach Bewertung:** Zeige nur Clips mit Score > X
- **Ranking-Algorithmus:** Sortiere Clips nach Viral-Potenzial

### 5. **Video-Bearbeitung & Export**
- **Clip-Bearbeitung:**
  - Trimmen (Start/End-Frames)
  - Crop-Funktion (Bildausschnitt)
  - Geschwindigkeit anpassen (0.5x - 2x)
  - Text-Overlay hinzufÃ¼gen
  - Logo/Watermark-Integration
  - ÃœbergÃ¤nge zwischen Clips

- **Audio-Verarbeitung:**
  - Musik extrahieren / Audio isolieren
  - Musik zu Clips hinzufÃ¼gen
  - Volume normalisieren
  - Audio-Fade In/Out

- **Export-Optionen:**
  - Mehrere Formate: MP4 (H.264/H.265), WebM, MKV
  - ZielauflÃ¶sungen: 720p, 1080p, 4K
  - Optimierte Presets fÃ¼r Social Media:
    - YouTube Shorts (1080x1920, 9:16)
    - TikTok (1080x1920, 9:16)
    - Instagram Reels (1080x1920, 9:16)
    - Instagram Feed (1080x1080, 1:1)
    - Twitter/X (1280x720, 16:9)
  - QualitÃ¤ts-Presets: Low, Medium, High, Ultra

### 6. **Batch-Verarbeitung**
- Verarbeite mehrere Videos gleichzeitig
- Queue-System fÃ¼r groÃŸe Projekte
- Fortschrittsanzeige fÃ¼r jedes Video
- Fehlerbehandlung und Logs

### 7. **Lokale KI-Integration**

#### **Ollama Integration:**
- Starten/Stoppen von Ollama-Server
- Model Management (Download, List, Delete)
- Nutzbare Modelle:
  - Text-Analyse: llama3.1, mistral, neural-chat
  - Vision (optional): llava (fÃ¼r Bildanalyse)
- API-Calls fÃ¼r:
  - Szenenbeschreibung generieren
  - Keywords/Tags extrahieren
  - Engagements-Score basierend auf Inhaltsanalyse

#### **Whisper Integration (lokale Spracherkennung):**
- Audio aus Video extrahieren
- Transkription ohne Cloud
- Timestamps fÃ¼r Sprachpassagen
- Genutzt fÃ¼r:
  - Moment-Erkennung (laute Momente)
  - Text-basierte Szenenerkennung
  - Automatische Captions generieren

### 8. **BenutzeroberflÃ¤che (UI/UX)**
- **Tab 1: Upload & Konfiguration**
  - Video-Datei auswÃ¤hlen
  - Erkennungs-Parameter einstellen
  - Start-Button
  
- **Tab 2: Scene Detection & Vorschau**
  - Timeline mit erkannten Szenen
  - Video-Vorschau
  - Thumbnail-Grid
  - Interaktive Einstellung (Schwellenwerte in Echtzeit anpassen)

- **Tab 3: Clip-Liste & Management**
  - Tabelle mit allen generierten Clips
  - Spalten: Start, Dauer, Score, Tags, Vorschau-Button
  - Filterbar nach Score, LÃ¤nge, Tags
  - Sortierbar nach Score, LÃ¤nge, Position

- **Tab 4: Clip-Editor**
  - Video-Player mit Schnitt-Tools
  - Bearbeitungs-Optionen (Trim, Crop, Speed, Text, Logo)
  - Live-Vorschau
  - Undo/Redo

- **Tab 5: Batch Export**
  - WÃ¤hle Clips zum Exportieren
  - Format/QualitÃ¤t Einstellungen
  - Presets fÃ¼r Social Media
  - Export-Progress
  - Open Output Folder Button

- **Tab 6: Settings & Tools**
  - Ollama Server Status & Control
  - Whisper Modell-Download
  - Speicher-Pfade konfigurieren
  - Advanced Scene Detection Settings
  - Logs anschauen
  - System-Informationen (GPU, RAM)

### 9. **Erweiterte Features**

#### **KI-gestÃ¼tzte Moment-Erkennung:**
- Kombiniere PySceneDetect + Whisper + Ollama
- Erkenne "virale" Momente:
  - PlÃ¶tzliche Reaktionen
  - Punchlines oder Witze
  - Musik-Beats
  - Dramatische Momente

#### **Auto-Tagging:**
- Tags automatisch basierend auf:
  - Visuelle Inhaltsanalyse (Ollama Vision)
  - Audio-Inhalt (Whisper Transkription)
  - Szenenerkennung
- Nutzerbare Tags fÃ¼r Filter/Organisation

#### **Clip-Vorlagen:**
- Speichere Bearbeitungs-Einstellungen als Vorlagen
- Quick-Apply auf neue Clips
- Standard-Vorlagen fÃ¼r verschiedene Inhaltstypen

#### **Performance-Optimierung:**
- Multi-Threading fÃ¼r Video-Verarbeitung
- GPU-Beschleunigung (wenn verfÃ¼gbar)
- Caching von analysierten Frames
- Fortlaufende Verarbeitung im Hintergrund

---

## ğŸ› ï¸ Technologie-Stack

### **Backend:**
- **Python 3.9+**
- **PySceneDetect** fÃ¼r Scene Detection
- **FFmpeg/PyAV** fÃ¼r Video-Verarbeitung
- **Ollama** (lokal laufender LLM-Server) fÃ¼r KI-Features
- **Whisper** (OpenAI's offline speech-to-text)
- **NumPy, Pillow** fÃ¼r Bild-/Frame-Verarbeitung

### **Frontend:**
- **Tkinter** (Desktop) ODER **PyQt6** (professionellere UI) ODER **FastAPI + React/Vue** (Web-Interface)
  - *Empfehlung:* PyQt6 fÃ¼r professionelle Desktop-App mit modernem Design
  
### **Dateistruktur:**
```
opusclips-offline/
â”œâ”€â”€ main.py                          # Entry Point
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ main_window.py              # Haupt-Fenster
â”‚   â”œâ”€â”€ tabs/
â”‚   â”‚   â”œâ”€â”€ upload_tab.py           # Upload & Config
â”‚   â”‚   â”œâ”€â”€ detection_tab.py        # Scene Detection
â”‚   â”‚   â”œâ”€â”€ clips_tab.py            # Clip-Liste
â”‚   â”‚   â”œâ”€â”€ editor_tab.py           # Clip-Editor
â”‚   â”‚   â”œâ”€â”€ export_tab.py           # Batch Export
â”‚   â”‚   â””â”€â”€ settings_tab.py         # Einstellungen
â”‚   â””â”€â”€ widgets/
â”‚       â”œâ”€â”€ video_player.py         # Video-Player Widget
â”‚       â”œâ”€â”€ timeline_widget.py      # Scene Timeline
â”‚       â””â”€â”€ progress_widget.py      # Progress Bar
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ scene_detection.py          # PySceneDetect Wrapper
â”‚   â”œâ”€â”€ video_processor.py          # FFmpeg Wrapper
â”‚   â”œâ”€â”€ clip_generator.py           # Clip-Generierungs-Logik
â”‚   â”œâ”€â”€ clip_ranker.py              # Bewertungs-Algorithmus
â”‚   â”œâ”€â”€ ai_integration.py           # Ollama + Whisper Integration
â”‚   â”œâ”€â”€ metadata_handler.py         # JSON Metadaten speichern
â”‚   â””â”€â”€ config.py                   # Konfigurationshandler
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ ffmpeg_utils.py             # FFmpeg Commands
â”‚   â”œâ”€â”€ ollama_client.py            # Ollama API Client
â”‚   â”œâ”€â”€ whisper_handler.py          # Whisper Integration
â”‚   â”œâ”€â”€ logger.py                   # Logging System
â”‚   â””â”€â”€ validators.py               # Input Validation
â”œâ”€â”€ requirements.txt                 # Python Dependencies
â”œâ”€â”€ README.md                        # Dokumentation
â””â”€â”€ config.json                      # Standard-Config

```

---

## ğŸš€ Implementierungs-Schritte (fÃ¼r Copilot)

### **Phase 1: GrundgerÃ¼st**
1. Erstelle Projekt-Struktur
2. Implementiere Konfigurationssystem
3. Baue Basis-UI mit PyQt6
4. Verbinde FFmpeg fÃ¼r Basic Video-Info

### **Phase 2: Video-Analyse**
1. Integriere PySceneDetect
2. Implementiere Scene Detection
3. Frame-Extraction & Thumbnail-Generierung
4. Zeige Timeline in UI

### **Phase 3: Clip-Generierung**
1. Automatische Clip-Generierung basierend auf Szenen
2. Intelligente LÃ¤ngen-Anpassung
3. Clip-Ranking & Bewertung
4. Speichere Metadaten (JSON)

### **Phase 4: KI-Integration**
1. Ollama Server Control (start/stop)
2. Whisper Spracherkennung
3. Intelligente Moment-Erkennung
4. Auto-Tagging

### **Phase 5: Editor & Export**
1. Video-Editor mit Trimmen/Crop
2. Text-Overlay & Logo-Funktionen
3. Multi-Format Export (MP4, WebM, MKV)
4. Social Media Presets

### **Phase 6: Batch-Verarbeitung & Optimierung**
1. Queue-System
2. Multi-Threading
3. GPU-Beschleunigung
4. Caching-System

---

## ğŸ’¡ Spezifische Code-Anforderungen

### **Scene Detection Module:**
```
- Funktion: `detect_scenes(video_path, sensitivity=0.3, detector_type='adaptive')`
  RÃ¼ckgabe: List[(start_time_seconds, end_time_seconds, confidence_score)]

- Funktion: `get_scene_thumbnails(video_path, scene_list, quality='medium')`
  RÃ¼ckgabe: Dict[scene_id -> PIL.Image]

- Funktion: `adjust_sensitivity_realtime(video_path, new_sensitivity)`
  Echtzeitanpassung des Erkennungsalgorithmus
```

### **Clip Generator Module:**
```
- Klasse: `ClipGenerator`
  - Methode: `generate_clips_from_scenes(scenes, target_duration=30, min_duration=10, max_duration=60)`
  - Methode: `auto_extend_short_clips(clip_list)`
  - Methode: `merge_overlapping_clips(clip_list)`
  
- Klasse: `ClipRanker`
  - Methode: `score_clip(clip, frame_data)`  # Score 1-10
  - Methode: `extract_features(clip)` # [speed_score, sound_score, complexity]
  - Methode: `rank_by_virality(clip_list)` # Sortierte Liste
```

### **AI Integration Module:**
```
- Klasse: `OllamaIntegrator`
  - Methode: `start_server()` / `stop_server()`
  - Methode: `analyze_scene_content(frame_list)` # Beschreibung generieren
  - Methode: `extract_keywords(scene_description)` # Tags generieren
  - Methode: `rate_virality(scene_analysis)` # 1-10 Score

- Klasse: `WhisperIntegrator`
  - Methode: `transcribe_audio(video_path)` # [(time, text)]
  - Methode: `detect_loud_moments(audio, threshold_db)` # [(start, end)]
  - Methode: `generate_captions(transcription, subtitle_format='srt')`
```

### **Video Editor Module:**
```
- Klasse: `VideoEditor`
  - Methode: `trim_clip(input_path, start_frame, end_frame, output_path)`
  - Methode: `crop_video(input_path, x, y, width, height, output_path)`
  - Methode: `change_speed(input_path, speed_factor, output_path)` # 0.5 - 2.0
  - Methode: `add_text_overlay(input_path, text, position, duration, output_path)`
  - Methode: `add_logo(input_path, logo_path, position, size, output_path)`
  - Methode: `add_transition(clip1, clip2, transition_type='fade', duration=0.5)`
```

### **Export Module:**
```
- Klasse: `VideoExporter`
  - Methode: `export_batch(clips, format='mp4', quality='high', preset='youtube_shorts')`
  - Methode: `get_social_media_presets()` # Dict mit Presets
  - Methode: `normalize_audio(clip_list)` # Level-Normalisierung
  - Methode: `verify_export_quality(exported_video_path)`
```

---

## ğŸ“Š Bewertungs-Algorithmus (Beispiel)

```python
def calculate_clip_score(clip_data):
    """
    Berechne Viral-Score fÃ¼r einen Clip (1-10)
    
    Faktoren:
    - scene_cut_speed: Wie schnell wechseln Szenen (0-3 Punkte)
    - audio_dynamics: LautstÃ¤rke-Spitzen (0-3 Punkte)
    - scene_complexity: Visuelle KomplexitÃ¤t (0-2 Punkte)
    - manual_boost: Nutzer-Bewertung (-1 bis +1 Punkte)
    """
    
    base_score = (
        scene_cut_speed * 3 +
        audio_dynamics * 3 +
        scene_complexity * 2
    ) / 8  # Normalisiert auf 0-1
    
    final_score = (base_score * 10) + manual_boost
    return max(1, min(10, final_score))  # Clamp 1-10
```

---

## ğŸ”§ Ollama & Whisper Setup (fÃ¼r Anwender)

### **Ollama Installation:**
```bash
# macOS
brew install ollama
ollama run llama3.1

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
ollama run llama3.1

# Windows
# Download von: https://ollama.ai/download
```

### **Whisper Installation:**
```bash
pip install openai-whisper
whisper --help
```

---

## âš™ï¸ Konfigurationsbeispiel (config.json)

```json
{
  "video": {
    "supported_formats": ["mp4", "mkv", "webm", "mov"],
    "max_file_size_gb": 50,
    "temp_folder": "./temp"
  },
  "scene_detection": {
    "default_sensitivity": 0.3,
    "adaptive_detector": true,
    "save_keyframes": true
  },
  "clip_generation": {
    "default_target_duration": 30,
    "min_duration": 10,
    "max_duration": 60,
    "auto_extend_short_clips": true
  },
  "ai_models": {
    "ollama_enabled": true,
    "ollama_host": "http://localhost:11434",
    "ollama_model": "llama3.1",
    "whisper_model": "base",
    "whisper_device": "auto"
  },
  "export": {
    "default_format": "mp4",
    "default_quality": "high",
    "default_preset": "youtube_shorts",
    "use_gpu": true
  },
  "ui": {
    "theme": "dark",
    "window_width": 1400,
    "window_height": 900,
    "font_size": 11
  }
}
```

---

## âœ… Quality Assurance Checklist

- [ ] Alle Videos mit unterschiedlichen Codecs funktionieren
- [ ] Scene Detection ist zuverlÃ¤ssig (teste verschiedene Schwellenwerte)
- [ ] Clips sind korrekt geschnitten (Keyframes korrekt)
- [ ] Ratings sind konsistent und nachvollziehbar
- [ ] Ollama lÃ¤uft stabil im Hintergrund
- [ ] Whisper transkribiert prÃ¤zise
- [ ] Exporte sind in allen Social-Media-Formaten korrekt
- [ ] Performance ist akzeptabel (GPU-Nutzung)
- [ ] UI ist responsive, auch bei groÃŸen Videos
- [ ] Error-Handling ist umfassend
- [ ] Logs sind hilfreich und detailliert

---

## ğŸ“ ZusÃ¤tzliche Informationen fÃ¼r Copilot

- Verwende **Type Hints** Ã¼berall (Python 3.9+)
- Schreibe **Docstrings** fÃ¼r alle Funktionen
- Verwende **Logging** statt Print-Statements
- Implementiere **Error Handling** mit aussagekrÃ¤ftigen Messages
- Erstelle **Unit Tests** fÃ¼r kritische Funktionen
- Nutze **Async/Await** fÃ¼r lange Operationen wo sinnvoll
- Optimiere fÃ¼r **CPU und GPU** Performance
- Dokumentiere **alle Konfigurationsoptionen**
- Achte auf **Memory-Leaks** bei groÃŸen Videodateien
- Teste mit **Videos verschiedener LÃ¤ngen** (5min bis 2h+)

---

## ğŸš€ Start-Befehl fÃ¼r Copilot Chat

**Nutze diesen Prompt am Anfang:**

> "Erstelle ein vollstÃ¤ndiges Python-Projekt basierend auf dem OpusClips-Offline-Spezifikation-Dokument. Starte mit Phase 1 (GrundgerÃ¼st + Projekt-Struktur). Verwende PyQt6 fÃ¼r die UI, PySceneDetect fÃ¼r Scene Detection. Erstelle alle Dateien, die fÃ¼r die erste Phase notwendig sind, mit vollstÃ¤ndigem, produktionsfertigem Code. Schreibe Docstrings, Type Hints und Error Handling Ã¼berall. ErklÃ¤re deine Architektur-Entscheidungen."

Dann fÃ¼r jede weitere Phase:
> "Implementiere Phase [N] des OpusClips-Offline-Projekts. Verwende die bestehende Struktur und integriere mit vorherigen Phasen. Schreibe produktionsfertigen Code mit Type Hints und Docstrings."

---

**Ende des Prompts - Viel Erfolg mit der Entwicklung! ğŸ¬ğŸš€**
