# GitHub Copilot Chat Prompts f√ºr OpusClips Offline

Nutze diese Prompts direkt in GitHub Copilot Chat oder anderen AI-Coding-Tools.

---

## Phase 1: Projekt-Setup & Grundger√ºst

### Prompt 1.1: Projekt-Struktur erstellen

```
Erstelle die komplette Projekt-Struktur f√ºr "opusclips-offline" mit allen Ordnern und Python-Dateien gem√§√ü der Spezifikation. F√ºr jede Python-Datei erstelle auch eine leere `__init__.py`. Gib mir die genaue Ordnerstruktur aus und erkl√§re den Zweck jeder Komponente.
```

### Prompt 1.2: Requirements.txt und Setup

```
Schreibe die `requirements.txt` Datei mit allen notwendigen Dependencies f√ºr OpusClips Offline:
- PyQt6 f√ºr GUI
- scenedetect f√ºr Scene Detection
- opencv-python f√ºr Video-Verarbeitung
- Pillow f√ºr Bildverarbeitung
- requests f√ºr API-Calls
- numpy f√ºr Numerik
- ffmpeg-python f√ºr FFmpeg-Wrapper
- ollama f√ºr lokale LLM-Integration
- openai-whisper f√ºr Spracherkennung
- python-dotenv f√ºr Konfiguration

Gib auch die genauen Versionsangaben an.
```

### Prompt 1.3: Config-System

```
Schreibe eine `config.py` Datei mit einer `Config` Klasse, die:
1. `config.json` l√§dt und parsed
2. Default-Werte bereitstellt falls config.json nicht existiert
3. Umgebungsvariablen √ºberschreiben k√∂nnen
4. Getter-Methoden f√ºr alle Config-Werte hat
5. Type Hints und Docstrings hat

Die Config soll folgende Bereiche unterst√ºtzen:
- video (supported_formats, max_file_size_gb, temp_folder)
- scene_detection (default_sensitivity, adaptive_detector)
- clip_generation (default_target_duration, min_duration, max_duration)
- ai_models (ollama_enabled, ollama_host, whisper_model)
- export (default_format, default_quality, use_gpu)
- ui (theme, window_dimensions, font_size)

Schreibe auch einen logger mit Python's logging Modul.
```

### Prompt 1.4: Basis-PyQt6 Fenster

```
Erstelle die Hauptanwendung in `ui/main_window.py` mit PyQt6:
1. Ein QMainWindow mit Dark-Theme
2. Ein QTabWidget mit 6 Tabs (Upload, Detection, Clips, Editor, Export, Settings)
3. Jeder Tab ist leer aber als Placeholder mit Label
4. Implementiere Men√ºbar mit File (Open, Exit) und Help (About)
5. Status Bar am unten
6. Responsive Layout mit Splitter wo sinnvoll
7. Speichere Window-Gr√∂√üe in config
8. Schreibe vollst√§ndigen Code mit Type Hints und Docstrings

Nutze auch das Logger-System f√ºr Debug-Infos.
```

### Prompt 1.5: FFmpeg Wrapper

```
Erstelle `utils/ffmpeg_utils.py` mit Funktionen f√ºr:
1. `get_video_info(video_path: str) -> dict` - Dauer, Aufl√∂sung, FPS, Bitrate
2. `extract_frame(video_path: str, timestamp_sec: float) -> PIL.Image`
3. `cut_video(input_path: str, start_sec: float, end_sec: float, output_path: str) -> bool`
4. `get_total_frames(video_path: str) -> int`
5. `check_ffmpeg_installed() -> bool`

Nutze subprocess oder ffmpeg-python. Schreibe Error Handling und Type Hints.
```

---

## Phase 2: Video-Analyse & Scene Detection

### Prompt 2.1: Scene Detection Wrapper

```
Erstelle `core/scene_detection.py` mit einer `SceneDetector` Klasse:

1. Methode `detect_scenes(video_path: str, sensitivity: float = 0.3) -> List[Tuple[float, float, float]]`
   - R√ºckgabe: [(start_sec, end_sec, confidence), ...]
   - Nutze PySceneDetect mit AdaptiveDetector
   - Handle verschiedene Video-Formate

2. Methode `adjust_sensitivity(new_sensitivity: float) -> None`
   - √Ñndere Erkennungsempfindlichkeit f√ºr Echtzeit-Tests

3. Methode `get_scene_info() -> dict`
   - Gib aktuelle Szenen-Daten zur√ºck

Schreibe auch Unit Tests f√ºr die Erkennung mit Test-Videos.
```

### Prompt 2.2: Thumbnail-Generierung

```
Erstelle in `core/scene_detection.py` die Methode:

`generate_thumbnails(video_path: str, scenes: List[Tuple[float, float, float]], quality: str = 'medium') -> Dict[int, PIL.Image]`

1. F√ºr jede Szene extrahiere die mittlere Sekunde als Thumbnail
2. Qualit√§t: 'low' (320x180), 'medium' (640x360), 'high' (1280x720)
3. Caching der Thumbnails im Temp-Folder
4. Gib Dictionary mit scene_id -> PIL.Image zur√ºck
5. Nutze FFmpeg oder OpenCV

Optimiere f√ºr Performance (parallel processing wenn m√∂glich).
```

### Prompt 2.3: Detection Tab UI

```
Implementiere den "Detection" Tab in `ui/tabs/detection_tab.py`:

1. Oben: Video-Name, Dauer, Aufl√∂sung, Anzahl Szenen
2. Mitte: Slider f√ºr Sensitivity (0.1 - 0.9)
   - Live-Update bei √Ñnderungen
   - "Re-detect" Button
3. Timeline-Widget mit erkannten Szenen
   - Zeige Thumbnails
   - Hover = Tooltip mit Zeit
   - Click = Video-Position spulen
4. Unten: "Export Scenes as CSV" Button

Nutze Threading f√ºr Re-detection ohne Einfrieren der UI.
```

### Prompt 2.4: Timeline Widget

```
Erstelle `ui/widgets/timeline_widget.py` mit `TimelineWidget` Klasse:

1. Zeige horizontale Timeline mit Szenen-Bl√∂cken
2. Jeder Block = 1 Szene mit Thumbnail
3. Hover-Effekt (helleres Thumbnail)
4. Click-Event zum Navigieren im Video-Player
5. Scrollbar wenn mehr Szenen als Platz
6. F√§rbe Bl√∂cke basierend auf L√§nge/Score
7. Custom QWidget mit paintEvent f√ºr Rendering

Optional: Zeige auch Audio-Waveform als Hintergrund.
```

---

## Phase 3: Intelligente Clip-Generierung

### Prompt 3.1: Clip Generator

```
Erstelle `core/clip_generator.py` mit `ClipGenerator` Klasse:

1. Methode `generate_clips_from_scenes(scenes: List[Tuple[float, float, float]], 
                                       target_duration: float = 30,
                                       min_duration: float = 10,
                                       max_duration: float = 60) -> List[Dict]`
   - Gruppiere Szenen in Clips der Zieldauer
   - Clips sollten nicht l√§nger als max_duration sein
   - Clips sollten nicht k√ºrzer als min_duration sein (au√üer letzte Szene)
   - R√ºckgabe: List[{start, end, duration, scene_indices}]

2. Methode `merge_short_clips(clips: List[Dict], min_duration: float = 10) -> List[Dict]`
   - Fusioniere zu kurze Clips mit Nachbarn

3. Methode `add_padding(clips: List[Dict], padding_ms: int = 0) -> List[Dict]`
   - Kann Padding zwischen Clips hinzuf√ºgen oder entfernen

Schreibe auch Tests f√ºr verschiedene Szenen-Kombinationen.
```

### Prompt 3.2: Clip Ranker

```
Erstelle `core/clip_ranker.py` mit `ClipRanker` Klasse:

1. Methode `extract_features(video_path: str, clip: Dict) -> Dict`
   - Extrahiere: scene_change_speed (0-1), audio_dynamics (0-1), visual_complexity (0-1)
   - scene_change_speed: Wie viele Schnitte pro Sekunde
   - audio_dynamics: RMS-Lautst√§rke-Spitzen (nutze FFmpeg)
   - visual_complexity: Histogram-Unterschiede zwischen Frames

2. Methode `calculate_score(features: Dict, manual_boost: float = 0) -> float`
   - Score = (scene_changes * 3 + audio * 3 + complexity * 2) / 8
   - Manual-Boost kann -1 bis +1 sein
   - Clamp auf 1-10 Skala

3. Methode `rank_clips(video_path: str, clips: List[Dict]) -> List[Dict]`
   - Score alle Clips und sortiere sie

Implementiere caching der Features um Performance zu sparen.
```

### Prompt 3.3: Metadata Handler

```
Erstelle `core/metadata_handler.py` mit Funktionen:

1. `save_project(project_name: str, video_path: str, scenes: List, clips: List, config: Dict) -> str`
   - Speichere alles in JSON mit timestamp
   - Gib Pfad zur gespeicherten JSON zur√ºck

2. `load_project(project_path: str) -> Dict`
   - Lade komplettes Projekt inkl. Szenen, Clips, etc.

3. `export_clips_csv(clips: List[Dict], output_path: str) -> None`
   - Exportiere Clips als CSV f√ºr externe Bearbeitung

4. `update_clip_metadata(project_path: str, clip_id: int, metadata: Dict) -> None`
   - Update einzelnen Clip nach Benutzer-Anpassung

Nutze JSON f√ºr Speicherung. Gib auch Versionsnummer rein f√ºr zuk√ºnftige Kompatibilit√§t.
```

### Prompt 3.4: Clips Tab UI

```
Implementiere "Clips" Tab in `ui/tabs/clips_tab.py`:

1. Oben: Filter-Optionen
   - Score Min/Max Slider
   - Length Min/Max Slider
   - Suchbox f√ºr Tags
   - Sort-Dropdown (Score, Length, Position)

2. Mitte: QTableWidget mit Spalten
   - # (Index)
   - Start Time
   - Duration
   - Score (mit Farbcode: rot < 5, gelb 5-7, gr√ºn > 7)
   - Tags (editable)
   - Action-Buttons (Preview, Edit, Delete)

3. Preview: Kleines Video-Player Widget wenn Row geklickt

4. Bottom: "Select All/None", "Delete Selected", "Refresh", "Save Project"

Nutze Threading f√ºr gro√üe Clip-Listen (100+).
```

---

## Phase 4: Lokale KI-Integration

### Prompt 4.1: Ollama Client

```
Erstelle `utils/ollama_client.py` mit `OllamaClient` Klasse:

1. Methode `start_server() -> bool`
   - Starte Ollama Server mit subprocess
   - Check ob bereits l√§uft
   - Return True/False

2. Methode `stop_server() -> bool`
   - Stoppe Ollama Server gracefully

3. Methode `is_running() -> bool`
   - Pr√ºfe via HTTP Request zu Ollama API

4. Methode `pull_model(model_name: str) -> bool`
   - Download Ollama Modell

5. Methode `list_models() -> List[str]`
   - Gib alle installierten Modelle zur√ºck

6. Methode `analyze_text(prompt: str, model: str = "llama3.1") -> str`
   - Sende Prompt zu Ollama und gib Response zur√ºck

Behandle Errors (Server nicht verf√ºgbar, etc.) mit aussagekr√§ftigen Messages.
```

### Prompt 4.2: Whisper Handler

```
Erstelle `utils/whisper_handler.py` mit `WhisperTranscriber` Klasse:

1. Methode `transcribe_video(video_path: str, model_size: str = "base") -> List[Dict]`
   - Extrahiere Audio aus Video
   - Transkribiere mit Whisper
   - R√ºckgabe: [{time: float, text: str, confidence: float}, ...]
   - Modell-Gr√∂√üe: "tiny" (39M), "base" (140M), "small" (244M)

2. Methode `detect_loud_moments(video_path: str, threshold_db: int = -20) -> List[Tuple[float, float]]`
   - Erkenne Momente mit lauter Sprache/Musik
   - Nutze ffmpeg um Audio zu analysieren
   - R√ºckgabe: [(start_sec, end_sec), ...]

3. Methode `get_speaker_segments(transcription: List[Dict]) -> List[Tuple[float, float]]`
   - Finde zusammenh√§ngende Sprach-Segmente

4. Methode `download_model(model_size: str) -> bool`
   - Download Whisper Modell wenn nicht vorhanden

Nutze Thread-Pool f√ºr Audio-Extraktion.
```

### Prompt 4.3: AI Integration Module

```
Erstelle `core/ai_integration.py` mit `AIAnalyzer` Klasse:

1. Methode `analyze_scene_for_keywords(frame: PIL.Image, ollama_client: OllamaClient) -> List[str]`
   - Nutze Ollama um Bild zu beschreiben (nutze llava Modell)
   - Extrahiere 5-10 Keywords
   - Return: ["keyword1", "keyword2", ...]

2. Methode `extract_emotional_moments(transcription: List[Dict]) -> List[Tuple[float, str]]`
   - Erkenne emotional erregte Momente aus Transkription
   - Return: [(timestamp, emotion_type), ...]
   - Nutze Ollama f√ºr Sentiment Analysis

3. Methode `calculate_virality_score(
       scene_changes: float,  # 0-1
       audio_peaks: float,     # 0-1
       keywords: List[str],    # k√∂nnte viral sein
       emotion_spikes: int,    # Anzahl emotionaler Momente
   ) -> float`  # 1-10
   - Kombiniere alle Faktoren zu Viral-Score
   - Nutze Gewichte: scene_changes (30%), audio (30%), keywords (20%), emotion (20%)

Schreibe Prompts die gut mit lokalen LLMs funktionieren (kurz, fokussiert).
```

### Prompt 4.4: Settings Tab - AI Configuration

```
Implementiere AI-Einstellungen in `ui/tabs/settings_tab.py`:

1. Sektion "Ollama Server"
   - Status-Label (Running / Stopped)
   - Start/Stop Buttons
   - Model Dropdown (liste verf√ºgbare Modelle)
   - Model-Download Button
   - Custom Host URL Input

2. Sektion "Whisper"
   - Model Size Dropdown (tiny, base, small)
   - Download Button
   - Test Transcription Button

3. Sektion "AI Analysis"
   - Checkbox "Enable Scene Analysis"
   - Checkbox "Enable Keyword Extraction"
   - Checkbox "Enable Emotion Detection"
   - Slider "Analysis Confidence Threshold"

4. Sektion "Performance"
   - Checkbox "Use GPU" (wenn verf√ºgbar)
   - CPU Threads Spinner (4-32)
   - RAM Limit Input

5. Log-Viewer
   - QTextEdit mit letzten 100 Log-Zeilen
   - Buttons: Clear, Save, Auto-Scroll Toggle

Nutze Threading f√ºr lange Operationen.
```

---

## Phase 5: Video-Editor & Export

### Prompt 5.1: Video Editor

```
Erstelle `core/video_editor.py` mit `VideoEditor` Klasse:

Methoden f√ºr jede Operation:

1. `trim_clip(input_path: str, start_frame: int, end_frame: int, output_path: str) -> bool`
   - Schneide Video auf exakte Frames

2. `crop_video(input_path: str, x: int, y: int, width: int, height: int, output_path: str) -> bool`
   - Croppt Video auf Ausschnitt

3. `change_speed(input_path: str, speed_factor: float, output_path: str) -> bool`
   - speed_factor: 0.5 (halb so schnell) bis 2.0 (doppelt so schnell)

4. `add_text_overlay(input_path: str, text: str, position: str, font_size: int, duration_sec: float, output_path: str) -> bool`
   - position: "top-left", "top-right", "bottom-left", "bottom-right", "center"
   - Nutze FFmpeg Filter (drawtext)

5. `add_logo(input_path: str, logo_path: str, position: str, size_percent: float, duration_sec: float, output_path: str) -> bool`
   - logo_path: Path zu Logo PNG/JPG
   - size_percent: 0-100% der Video-Breite
   - Nutze FFmpeg Overlay-Filter

6. `add_fade_transition(input_path: str, duration_sec: float = 0.5, fade_type: str = "fade_in", output_path: str) -> bool`
   - fade_type: "fade_in", "fade_out", "fade_inout"

7. `normalize_audio(input_path: str, target_loudness_db: float = -14.0, output_path: str) -> bool`

Alle Funktionen nutzen FFmpeg √ºber subprocess. Schreibe Error Handling und Progress-Callbacks.
```

### Prompt 5.2: Video Player Widget

```
Erstelle `ui/widgets/video_player.py` mit `VideoPlayerWidget` Klasse (QWidget):

1. Zeige Video mit OpenCV (oder FFmpeg)
2. Playback Controls (Play, Pause, Stop, Seek-Bar)
3. Zeit-Anzeige (current_time / total_time)
4. Volume Slider (0-100%)
5. Speed Dropdown (0.5x, 1.0x, 1.5x, 2.0x)
6. Frame-by-Frame Navigation (‚Üê ‚Üí)
7. Keyboard Shortcuts (Space=Play/Pause, ‚Üê/‚Üí=Seek)
8. Auto-loop Option

Nutze Threading f√ºr Video-Decoding um UI nicht zu blocken.

Optional: Thumbnail-Preview beim Hover √ºber Seek-Bar.
```

### Prompt 5.3: Clip Editor Tab

```
Implementiere "Editor" Tab in `ui/tabs/editor_tab.py`:

1. Oben: Clip Selection Dropdown
   - Zeige alle verf√ºgbaren Clips

2. Mitte-Links: Video Player Widget
   - Zeige aktuellen Clip
   - Trim Handles (Start/End Drag Bar)

3. Mitte-Rechts: Edit Options
   - Trim: Start/End Time Spinners
   - Crop: X, Y, Width, Height Inputs + Visual Crop-Preview
   - Speed: Slider 0.5-2.0
   - Text Overlay: Text Input + Position Dropdown + Font Size
   - Logo: File Browser + Position + Size Slider
   - Fade: Type Dropdown + Duration

4. Unten: Preview Button, Save Changes Button, Reset Button

5. Status: "Unsaved changes" Indicator

Nutze Threading f√ºr Preview-Generierung.
```

### Prompt 5.4: Video Exporter

```
Erstelle `core/video_exporter.py` mit `VideoExporter` Klasse:

1. Methode `export_batch(clips: List[Dict], 
                        video_path: str,
                        format: str = "mp4",
                        quality: str = "high",
                        preset: str = "youtube_shorts",
                        output_folder: str) -> List[str]`
   - Exportiere alle Clips als separate Videos
   - Return: List[output_paths]

2. Methode `get_social_media_presets() -> Dict`
   - Return Dict mit Presets:
   ```python
   {
       "youtube_shorts": {height: 1080, width: 1920, ratio: "9:16", fps: 30},
       "tiktok": {height: 1080, width: 1920, ratio: "9:16", fps: 30},
       "instagram_reels": {height: 1080, width: 1920, ratio: "9:16", fps: 30},
       "instagram_feed": {height: 1080, width: 1080, ratio: "1:1", fps: 30},
       "twitter": {height: 720, width: 1280, ratio: "16:9", fps: 30},
   }
   ```

3. Methode `export_with_settings(
       input_path: str,
       output_path: str,
       format: str,
       quality_level: str,  # "low", "medium", "high", "ultra"
       preset: str,
       on_progress: Callable) -> bool`
   - on_progress: Callback mit (current, total, percentage)

4. Methode `verify_export(output_path: str) -> bool`
   - Check ob Datei g√ºltig, hat richtige Aufl√∂sung, etc.

Nutze FFmpeg mit optimierten Profiles f√ºr jede Plattform.
```

### Prompt 5.5: Export Tab UI

```
Implementiere "Export" Tab in `ui/tabs/export_tab.py`:

1. Oben: Clip Selection
   - Checkbox-Liste aller Clips
   - "Select All / None" Buttons
   - Filter nach Score/Length

2. Export Settings (Left Panel)
   - Format: MP4 / WebM / MKV Dropdown
   - Quality: Low / Medium / High / Ultra Dropdown
   - Social Media Preset: Dropdown mit Presets
   - Custom Resolution: Width/Height Spinners
   - FPS: Spinner
   - Bitrate: "Auto" oder Manual Input

3. Advanced (Collapsible)
   - Video Codec: Dropdown (h264, h265, VP9, AV1)
   - Audio Codec: Dropdown
   - Normalize Audio: Checkbox + dB Input
   - Add Watermark: Checkbox + File Browser

4. Mitte/Unten: Batch Export
   - "Start Export" Button (disabled wenn keine Clips ausgew√§hlt)
   - Progress Bar (gesamt und pro Clip)
   - Status-Text (Processing clip 3/10: file.mp4 - 45% done)
   - Output Folder Browser
   - "Open Output Folder" Button nach Export

5. Cancel Button (visible w√§hrend Export l√§uft)

Nutze Threading damit Export nicht UI blockt.
```

---

## Phase 6: Batch-Verarbeitung & Optimierung

### Prompt 6.1: Task Queue System

```
Erstelle `core/task_queue.py` mit `TaskQueue` Klasse:

1. Verwende Thread-Pool (ThreadPoolExecutor)
2. Jede Task hat Status: "pending", "processing", "completed", "failed"
3. Methode `add_task(task_type: str, params: Dict) -> str`
   - task_type: "detect_scenes", "generate_clips", "export_clip", "analyze_ai"
   - Return: task_id

4. Methode `get_task_status(task_id: str) -> Dict`
   - Return: {status, progress_percent, result/error}

5. Methode `cancel_task(task_id: str) -> bool`

6. Methode `get_all_tasks() -> List[Dict]`

7. Callback-System f√ºr Task-Updates
   - on_progress, on_complete, on_error Callbacks

Implementiere auch Persistierung: Speichere Task-Queue als JSON damit sie nach App-Restart wiederhergestellt werden kann.
```

### Prompt 6.2: Caching System

```
Erstelle `utils/cache_manager.py` mit `CacheManager` Klasse:

1. Cache f√ºr:
   - Extracted Frames (PIL.Image)
   - Generated Thumbnails
   - Scene Detection Results
   - Transcription Results
   - AI Analysis Results

2. Methode `set_cache(key: str, value: Any, ttl_minutes: int = 60) -> None`
   - TTL = Time-To-Live

3. Methode `get_cache(key: str) -> Any | None`

4. Methode `clear_cache(pattern: str | None = None) -> None`
   - Clear alles oder spezifische Pattern

5. Methode `get_cache_stats() -> Dict`
   - Zeige Cache-Size, Hit-Rate, etc.

Nutze SQLite oder einfach JSON-Dateien im Temp-Folder f√ºr Cache-Speicherung.

Optional: LRU-Cache um Speicher zu sparen.
```

### Prompt 6.3: Performance Monitoring

```
Erstelle `utils/performance_monitor.py` mit `PerformanceMonitor` Klasse:

1. Track CPU, Memory, GPU Usage √ºber Zeit
2. Track Operation-Zeiten f√ºr jede Funktion
3. Methode `start_monitoring() -> None`
4. Methode `stop_monitoring() -> None`
5. Methode `get_metrics() -> Dict`
   - Return: {cpu_percent, memory_mb, gpu_percent, current_operation, operation_time}
6. Methode `log_operation(operation_name: str, duration_sec: float)`

Nutze `psutil` Library. Zeige diese Metriken in der Settings Tab "Performance" Section.
```

### Prompt 6.4: GPU Acceleration

```
Erstelle `utils/gpu_utils.py`:

1. Funktion `is_gpu_available() -> bool`
   - Check ob NVIDIA GPU mit CUDA verf√ºgbar

2. Funktion `get_gpu_info() -> Dict | None`
   - Return: {gpu_name, vram_total_gb, vram_available_gb, compute_capability}
   - Nutze `torch.cuda` oder `pycuda`

3. Funktion `get_ffmpeg_gpu_flags() -> List[str]`
   - Return FFmpeg Flags f√ºr GPU-Encoding
   - Bei NVIDIA: ["-hwaccel", "cuda", "-hwaccel_output_format", "cuda"]

4. Nutze diese Flags in VideoExporter wenn GPU=True

Hinweis: GPU hilft vor allem bei Export (Video Encoding), weniger bei Scene Detection.
```

---

## Zus√§tzliche Prompts: Testing & Troubleshooting

### Testing Prompt

```
Schreibe Unit Tests f√ºr OpusClips Offline:

1. Test Scene Detection mit verschiedenen Videos
2. Test Clip Generation Algorithmen
3. Test Ranking/Scoring Logic
4. Test Ollama Integration (Mock wenn n√∂tig)
5. Test Whisper Integration
6. Test Video Export mit verschiedenen Formaten
7. Test UI Responsiveness

Nutze `pytest` Framework. Teste auch Edge Cases:
- 1-Frame Videos
- 10-Sekunden Videos vs 2-Stunden Videos
- Videos ohne Audio
- Sehr schnelle Szenenwechsel
- Keine Szenenwechsel (statisches Video)

Gib mir auch ein `conftest.py` f√ºr Test-Fixtures (Test-Videos generieren).
```

### Troubleshooting Prompt

```
Schreibe ein Troubleshooting-System:

1. H√§ufige Fehler und deren L√∂sungen
2. System-Kompatibilit√§ts-Check beim Start
3. Dependency-Check
4. FFmpeg-Check
5. Ollama-Verf√ºgbarkeit-Check
6. GPU-Support-Check

Wenn Fehler erkannt werden: Zeige dem Nutzer aussagekr√§ftige Fehlermeldung und L√∂sungsschritte.

Beispiel: "FFmpeg nicht gefunden. Installiere mit: brew install ffmpeg (macOS) / apt install ffmpeg (Linux) / Download von ffmpeg.org (Windows)"
```

---

## Best Practices f√ºr die Copilot-Nutzung

1. **Pro Prompt: 1 Feature/Komponente**
   - Nicht zu viel auf einmal fragen
   - Fokussiert auf spezifische Funktionalit√§t

2. **Iteratives Vorgehen**
   - Anfrage stellen
   - Code reviewen
   - Fragen zur Verbesserung stellen
   - Refinement

3. **Error Handling**
   - Immer nach Error-Handling fragen falls nicht erw√§hnt
   - Spezifische Exception-Typen nutzen

4. **Documentation**
   - Docstrings sind wichtig
   - Type Hints √ºberall
   - Kommentare f√ºr komplexe Logik

5. **Performance**
   - Nach Threading/Async fragen bei langen Operationen
   - Caching nicht vergessen
   - GPU-Support erw√§hnen wo m√∂glich

6. **Testing**
   - Immer Unit Tests f√ºr kritische Funktionen fordern
   - Edge Cases testen

---

## Vollst√§ndiger Workflow-Beispiel

1. Starten Sie mit: "Erstelle die Projekt-Struktur und main.py"
2. Dann: "Implementiere die Config-System"
3. Dann: "Baue die Haupt-UI in PyQt6"
4. Dann: "Integriere FFmpeg Wrapper"
5. Dann: "F√ºge Scene Detection hinzu"
... und so weiter Phase f√ºr Phase.

Am Ende werden Sie ein vollst√§ndig funktionales Projekt haben! üöÄ
